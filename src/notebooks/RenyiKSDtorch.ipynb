{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625d477f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install torch torchvision diffusers==0.24.0 transformers==4.35.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b97d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import DDPMPipeline\n",
    "from torchvision import datasets, transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4adb6d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "pipe = DDPMPipeline.from_pretrained(\"google/ddpm-cifar10-32\").to(device)\n",
    "pipe.set_progress_bar_config(disable=True)\n",
    "unet = pipe.unet.eval()\n",
    "\n",
    "def score_from_noise_pred(eps_pred, sigma):\n",
    "    return -(1.0 / sigma) * eps_pred\n",
    "\n",
    "@torch.no_grad()\n",
    "def map_sigma_to_t(sigma):\n",
    "    alphas_cumprod = pipe.scheduler.alphas_cumprod.to(device)\n",
    "    sigmas = torch.sqrt((1 - alphas_cumprod) / alphas_cumprod)\n",
    "    t = int((sigmas - sigma).abs().argmin().item())\n",
    "    return t\n",
    "\n",
    "@torch.no_grad()\n",
    "def score_fn(x, sigma=0.1):\n",
    "    t = map_sigma_to_t(sigma)\n",
    "    t_tensor = torch.full((x.shape[0],), t, device=x.device, dtype=torch.long)\n",
    "    eps_pred = unet(x, t_tensor).sample\n",
    "    return score_from_noise_pred(eps_pred, sigma)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2ef5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfm = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda z: z*2-1),\n",
    "])\n",
    "cifar = datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=tfm)\n",
    "loader = torch.utils.data.DataLoader(cifar, batch_size=64, shuffle=True, num_workers=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141cd111",
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_gauss(X, Y, sigma2=0.5):\n",
    "    # X: (n,d), Y: (m,d)\n",
    "    XX = (X**2).sum(dim=1, keepdim=True)\n",
    "    YY = (Y**2).sum(dim=1, keepdim=True)\n",
    "    dist2 = XX - 2*X@Y.T + YY.T\n",
    "    return torch.exp(-dist2 / (2 * sigma2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b368e108",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def select_renyi_landmarks_np(X, m, sigma2=0.5):\n",
    "    # X: torch tensor (n,d), returns numpy indices\n",
    "    Xn = X.detach().cpu().numpy()\n",
    "    n = len(Xn); m = min(m, n)\n",
    "    # Gaussian kernel Gram\n",
    "    XX = (Xn**2).sum(axis=1, keepdims=True)\n",
    "    dist2 = XX - 2*Xn@Xn.T + XX.T\n",
    "    K = np.exp(-dist2 / (2 * sigma2))\n",
    "    diag = np.diag(K)\n",
    "    row_sums = K.sum(axis=1)\n",
    "    first = int(np.argmin(row_sums))\n",
    "    selected = [first]\n",
    "    cross_sums = K[:, first].copy()\n",
    "    while len(selected) < m:\n",
    "        scores = 2 * cross_sums + diag\n",
    "        scores[selected] = np.inf\n",
    "        nxt = int(np.argmin(scores))\n",
    "        selected.append(nxt)\n",
    "        cross_sums += K[:, nxt]\n",
    "    return np.array(selected, dtype=int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff41e85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RenyiNystroemKSDTorch:\n",
    "    def __init__(self, score_fn, sigma2=0.5, m_fn=lambda n: int(4*torch.sqrt(torch.tensor(n)).item())):\n",
    "        self.score_fn = score_fn\n",
    "        self.sigma2 = sigma2\n",
    "        self.m_fn = m_fn\n",
    "\n",
    "    def h_p(self, X, Y):\n",
    "        # X: (n,d), Y: (m,d)\n",
    "        grad_logpX = self.score_fn(X)\n",
    "        grad_logpY = self.score_fn(Y)\n",
    "        gram_glogp = grad_logpX @ grad_logpY.T\n",
    "        K = k_gauss(X, Y, sigma2=self.sigma2)\n",
    "\n",
    "        # gradients of kernel\n",
    "        # For Gaussian kernel: grad_X k = -(X - Y)/sigma2 * k\n",
    "        # Use broadcasting\n",
    "        diff = X[:, None, :] - Y[None, :, :]\n",
    "        k_xy = K[:, :, None]\n",
    "        gradX = -(diff / self.sigma2) * k_xy  # (n,m,d)\n",
    "        gradY = -gradX\n",
    "\n",
    "        B = (gradX * grad_logpY[None, :, :]).sum(dim=2)\n",
    "        C = (gradY * grad_logpX[:, None, :]).sum(dim=2)\n",
    "        gradXY_sum = (-diff / self.sigma2 * gradY).sum(dim=2)  # trace of second derivative term\n",
    "\n",
    "        return K * gram_glogp + B + C + gradXY_sum\n",
    "\n",
    "    def compute_stat(self, X):\n",
    "        n = X.shape[0]\n",
    "        m = min(self.m_fn(n), n)\n",
    "        idx = select_renyi_landmarks_np(X, m, sigma2=self.sigma2)\n",
    "        idx = torch.as_tensor(idx, device=X.device)\n",
    "\n",
    "        H_mn = self.h_p(X[idx], X)\n",
    "        H_mm = H_mn[:, idx]\n",
    "        H_mm_inv = torch.linalg.pinv(H_mm)\n",
    "        beta = H_mn @ (torch.ones(n, 1, device=X.device) / n)\n",
    "        stat = (beta.T @ H_mm_inv @ beta).squeeze()\n",
    "        return stat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf9912d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ksd = RenyiNystroemKSDTorch(score_fn=score_fn, sigma2=0.5)\n",
    "\n",
    "x, _ = next(iter(loader))\n",
    "x = x.to(device)\n",
    "loss = ksd.compute_stat(x)\n",
    "print(\"KSD stat:\", loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f279b7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# placeholder generator for smoke test\n",
    "import torch.nn as nn\n",
    "\n",
    "G = nn.Sequential(\n",
    "    nn.Linear(128, 512), nn.ReLU(),\n",
    "    nn.Linear(512, 3*32*32), nn.Tanh()\n",
    ").to(device)\n",
    "\n",
    "opt = torch.optim.Adam(G.parameters(), lr=2e-4)\n",
    "\n",
    "z = torch.randn(64, 128, device=device)\n",
    "fake = G(z).view(-1, 3, 32, 32)\n",
    "loss = ksd.compute_stat(fake)\n",
    "loss.backward()\n",
    "opt.step()\n",
    "\n",
    "print(\"Generator step done, loss:\", loss.item())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ksd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
